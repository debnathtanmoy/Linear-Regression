# Linear-Regression
Regression analysis is a supervised learning algorithm that is used when you want to predict a continuous dependent variable from a number of independent variables. This method is mostly used for forecasting and finding out cause and relationship(continuous) between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. 
Simple linear regression is a type of regression analysis where the number of independent variables is one and there is a linear relationship between the independent(x) and dependent(y) variable. Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation y = B0 + B1 * x , where y is the dependent Variable, x is Independent variable and B0 is called the intercept because it determines where the line intercepts the y-axis. The B1 term is called the slope because it defines the slope of the line or how x translates into a y value before we add our bias. 
These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line. The distance from the line to the data point is called residual. In linear regression it draws lines through the data and then calculate the distance from line to the data point, square each distance and then add them up (sum of squared residuals). It repeats this process by creating multiple lines and calculating sum of squared residuals. And then finally the line with least sum of squared residuals is selected. This method of fitting the line is called least squares.



